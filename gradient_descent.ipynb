{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time, linear regression was good enough to figure out where the dividing line runs. Now, what if there are multiple lines, or lines with wild curves? Time for gradient descent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch as tc\n",
    "\n",
    "class GradientDescender:\n",
    "    def __init__(self,initial_dataset:Dataset):\n",
    "        self.dataset = initial_dataset\n",
    "\n",
    "NUMBER_OF_FEATURES = 4\n",
    "\n",
    "class GradientDataset(Dataset):\n",
    "    def __init__(self,data,labels):\n",
    "        super().__init__()\n",
    "        self.features:tc.Tensor = tc.tensor(data,dtype=tc.float32)\n",
    "        self.labels:tc.Tensor = tc.tensor(labels,dtype=tc.float32)\n",
    "    \n",
    "    def __getitem__(self,index) -> tuple[tc.Tensor, tc.Tensor]:\n",
    "        datapoint = self.features[index]\n",
    "        datalabel = self.labels[index]\n",
    "        return datapoint, datalabel\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "data = array([[ 0.77, -1.14],\n",
    "       [-0.33,  1.44],\n",
    "       [ 0.91, -3.07],\n",
    "       [-0.37, -1.91],\n",
    "       [-0.63, -1.53],\n",
    "       [ 0.39, -1.99],\n",
    "       [-0.49, -2.74],\n",
    "       [-0.68, -1.52],\n",
    "       [-0.1 , -3.43],\n",
    "       [-0.05, -1.95],\n",
    "       [ 3.88,  0.65],\n",
    "       [ 0.73,  2.97],\n",
    "       [ 0.83,  3.94],\n",
    "       [ 1.59,  1.25],\n",
    "       [ 1.14,  3.91],\n",
    "       [ 1.73,  2.8 ],\n",
    "       [ 1.31,  1.85],\n",
    "       [ 1.56,  3.85],\n",
    "       [ 1.23,  2.54],\n",
    "       [ 1.33,  2.03]])\n",
    "labels = array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "dataset = GradientDataset(data,labels)\n",
    "\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LogisticRegression(tc.nn.Module):\n",
    "    def __init__(self,feature_count:int):\n",
    "        super().__init__() #Magic incantation to ensure it's a Module.\n",
    "        self.l_linear:tc.nn.Linear = tc.nn.Linear(in_features=feature_count, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logit = self.l_linear(x)\n",
    "        probabilities = tc.sigmoid(logit)\n",
    "        return probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to do the training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(e/b) 0/1: 1.2851252555847168\n",
      "(e/b) 1/1: 0.9563752412796027\n",
      "(e/b) 2/1: 0.8644943237304688\n",
      "(e/b) 3/1: 0.5678653717041016\n",
      "(e/b) 4/1: 0.6574111580848694\n",
      "(e/b) 5/1: 0.4674594998359685\n",
      "(e/b) 6/1: 0.36210691928863525\n",
      "(e/b) 7/1: 0.48643189668655396\n",
      "(e/b) 8/1: 0.46297580003738403\n",
      "(e/b) 9/1: 0.3001677095890045\n",
      "(e/b) 10/1: 0.35435402393341064\n",
      "(e/b) 11/1: 0.2849898934364319\n",
      "(e/b) 12/1: 0.21365562081336975\n",
      "(e/b) 13/1: 0.20167095959186554\n",
      "(e/b) 14/1: 0.20654626190662384\n",
      "(e/b) 15/1: 0.23921974003314972\n",
      "(e/b) 16/1: 0.32767659425735474\n",
      "(e/b) 17/1: 0.22183279693126678\n",
      "(e/b) 18/1: 0.17049723863601685\n",
      "(e/b) 19/1: 0.1409577578306198\n",
      "(e/b) 20/1: 0.12470986694097519\n",
      "(e/b) 21/1: 0.17798857390880585\n",
      "(e/b) 22/1: 0.10546517372131348\n",
      "(e/b) 23/1: 0.15026587247848512\n",
      "(e/b) 24/1: 0.12561306357383728\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(2)\n",
    "#model() \n",
    "optimiser = tc.optim.SGD(model.parameters(),lr = 0.05)\n",
    "epochs = 25 #Let's split the difference for now~ \n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_id, (features,label) in enumerate(train_loader):\n",
    "        out:tc.Tensor = model(features)\n",
    "\n",
    "        entropy = tc.nn.functional.binary_cross_entropy(out,label.view(out.shape))\n",
    "        optimiser.zero_grad()\n",
    "        entropy.backward()\n",
    "        optimiser.step()\n",
    "\n",
    "        print(f\"(e/b) {epoch}/{batch_id}: {entropy}\",end=\"\\r\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy function next!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(to_test:LogisticRegression, datasource:DataLoader) -> float: #assuming the accuracy will be some 1 to 0 number\n",
    "    to_test.eval()\n",
    "    correct, total = 0, 0\n",
    "    for idx, (features, class_labels) in enumerate(datasource):\n",
    "        with tc.no_grad():\n",
    "            probs = to_test(features)\n",
    "            prediction = tc.where(probs > 0.5,1,0)\n",
    "            lab = class_labels.view(prediction.shape).to(prediction.dtype)\n",
    "            correct_pick = prediction == lab\n",
    "            correct += tc.sum(correct_pick)\n",
    "            total += len(correct_pick)\n",
    "    return float(correct) / float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called the _init_ dunder.\n",
      "Called _call_ dunder, with value 1234\n",
      "Called _getitem_ dunder, with index 9\n",
      "Called _getitem_ dunder, with coordinates (1, 2)\n"
     ]
    }
   ],
   "source": [
    "class DunderExample:\n",
    "    def __init__(self):\n",
    "        print(\"Called the _init_ dunder.\")\n",
    "\n",
    "    def __call__(self,value:int):\n",
    "        print(f\"Called _call_ dunder, with value {value}\")\n",
    "    \n",
    "    def __getitem__(self,name):\n",
    "        if isinstance(name,tuple):\n",
    "            print(f\"Called _getitem_ dunder, with coordinates {name}\")\n",
    "        else:\n",
    "            print(f\"Called _getitem_ dunder, with index {name}\")\n",
    "\n",
    "example = DunderExample()\n",
    "example(1234)\n",
    "\n",
    "example[9]\n",
    "example[1,2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_project-ZtUy7KAZ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
